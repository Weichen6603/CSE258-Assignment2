{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "8tNiIOdw71g2",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:24.691898Z",
     "start_time": "2024-12-01T01:41:24.108346Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1=pd.read_csv('dataset/tmdb_5000_credits.csv')\n",
    "df2=pd.read_csv('dataset/tmdb_5000_movies.csv')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XNMXjSnrB-RB",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:24.719734Z",
     "start_time": "2024-12-01T01:41:24.695265Z"
    }
   },
   "source": [
    "df1.columns = ['id','tittle','cast','crew']\n",
    "df2= df2.merge(df1,on='id')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "xiiCUlAOCoej",
    "outputId": "3ee3f4c8-5851-4132-f28e-a7f069972e5a",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:24.970132Z",
     "start_time": "2024-12-01T01:41:24.958802Z"
    }
   },
   "source": [
    "df2['overview'].head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    In the 22nd century, a paraplegic Marine is di...\n",
       "1    Captain Barbossa, long believed to be dead, ha...\n",
       "2    A cryptic message from Bond’s past sends him o...\n",
       "3    Following the death of District Attorney Harve...\n",
       "4    John Carter is a war-weary, former military ca...\n",
       "Name: overview, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1_Uf0U0LbSg"
   },
   "source": "# Data processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOr0ArdOLdYe"
   },
   "source": "## Process plot descriptions"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aCbkqJWC-As",
    "outputId": "a46ec68f-da47-4bb6-d5a5-e3b6ec7d9ea5",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:25.906834Z",
     "start_time": "2024-12-01T01:41:25.057416Z"
    }
   },
   "source": [
    "# Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "df2['overview'] = df2['overview'].fillna('')\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(df2['overview'])\n",
    "\n",
    "# Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4803, 20978)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vSRuusC1C_VS",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:26.179321Z",
     "start_time": "2024-12-01T01:41:25.977198Z"
    }
   },
   "source": [
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2AB-5F-8DEfi",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:26.230082Z",
     "start_time": "2024-12-01T01:41:26.223333Z"
    }
   },
   "source": [
    "# Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7hc9xqEJXqI"
   },
   "source": "## Process Credits, Genres and Keywords"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2q1RhmL1DTfz",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.448633Z",
     "start_time": "2024-12-01T01:41:26.302378Z"
    }
   },
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    df2[feature] = df2[feature].apply(literal_eval)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UeKkEwE1DWWQ",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.509995Z",
     "start_time": "2024-12-01T01:41:30.498171Z"
    }
   },
   "source": [
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fHfgNdPqDXAK",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.576297Z",
     "start_time": "2024-12-01T01:41:30.568038Z"
    }
   },
   "source": [
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xf0IUVtxDaie",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.712787Z",
     "start_time": "2024-12-01T01:41:30.626699Z"
    }
   },
   "source": [
    "df2['director'] = df2['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    df2[feature] = df2[feature].apply(get_list)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "NYSOUaIGDdWk",
    "outputId": "1c298c4b-5da8-4a40-c93a-1559814faca6",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.757358Z",
     "start_time": "2024-12-01T01:41:30.747166Z"
    }
   },
   "source": [
    "df2[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                      title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "2                                   Spectre   \n",
       "\n",
       "                                               cast        director  \\\n",
       "0  [Sam Worthington, Zoe Saldana, Sigourney Weaver]   James Cameron   \n",
       "1     [Johnny Depp, Orlando Bloom, Keira Knightley]  Gore Verbinski   \n",
       "2      [Daniel Craig, Christoph Waltz, Léa Seydoux]      Sam Mendes   \n",
       "\n",
       "                              keywords                        genres  \n",
       "0   [culture clash, future, space war]  [Action, Adventure, Fantasy]  \n",
       "1   [ocean, drug abuse, exotic island]  [Adventure, Fantasy, Action]  \n",
       "2  [spy, based on novel, secret agent]    [Action, Adventure, Crime]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>director</th>\n",
       "      <th>keywords</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>[Sam Worthington, Zoe Saldana, Sigourney Weaver]</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>[culture clash, future, space war]</td>\n",
       "      <td>[Action, Adventure, Fantasy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[Johnny Depp, Orlando Bloom, Keira Knightley]</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>[ocean, drug abuse, exotic island]</td>\n",
       "      <td>[Adventure, Fantasy, Action]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>[Daniel Craig, Christoph Waltz, Léa Seydoux]</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>[spy, based on novel, secret agent]</td>\n",
       "      <td>[Action, Adventure, Crime]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ndi-Mf-aDlre",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.910161Z",
     "start_time": "2024-12-01T01:41:30.901675Z"
    }
   },
   "source": [
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "381lDqziDmbV",
    "outputId": "d10909a2-073d-442a-d1f2-194030bc8a29",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:30.995782Z",
     "start_time": "2024-12-01T01:41:30.973342Z"
    }
   },
   "source": [
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    df2[feature] = df2[feature].apply(clean_data)\n",
    "    print(df2[feature].head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [samworthington, zoesaldana, sigourneyweaver]\n",
      "1       [johnnydepp, orlandobloom, keiraknightley]\n",
      "2        [danielcraig, christophwaltz, léaseydoux]\n",
      "3        [christianbale, michaelcaine, garyoldman]\n",
      "4      [taylorkitsch, lynncollins, samanthamorton]\n",
      "Name: cast, dtype: object\n",
      "0       [cultureclash, future, spacewar]\n",
      "1       [ocean, drugabuse, exoticisland]\n",
      "2       [spy, basedonnovel, secretagent]\n",
      "3    [dccomics, crimefighter, terrorist]\n",
      "4        [basedonnovel, mars, medallion]\n",
      "Name: keywords, dtype: object\n",
      "0        jamescameron\n",
      "1       goreverbinski\n",
      "2           sammendes\n",
      "3    christophernolan\n",
      "4       andrewstanton\n",
      "Name: director, dtype: object\n",
      "0           [action, adventure, fantasy]\n",
      "1           [adventure, fantasy, action]\n",
      "2             [action, adventure, crime]\n",
      "3                 [action, crime, drama]\n",
      "4    [action, adventure, sciencefiction]\n",
      "Name: genres, dtype: object\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TznzWxTGDoJm",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:31.241943Z",
     "start_time": "2024-12-01T01:41:31.191136Z"
    }
   },
   "source": [
    "def create_soup(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
    "df2['soup'] = df2.apply(create_soup, axis=1)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ia0sRdZbDrGO",
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:31.320292Z",
     "start_time": "2024-12-01T01:41:31.296462Z"
    }
   },
   "source": [
    "print(df2[['id', 'title', 'cast', 'director', 'keywords', 'genres', 'soup']].head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                     title  \\\n",
      "0   19995                                    Avatar   \n",
      "1     285  Pirates of the Caribbean: At World's End   \n",
      "2  206647                                   Spectre   \n",
      "3   49026                     The Dark Knight Rises   \n",
      "4   49529                               John Carter   \n",
      "\n",
      "                                            cast          director  \\\n",
      "0  [samworthington, zoesaldana, sigourneyweaver]      jamescameron   \n",
      "1     [johnnydepp, orlandobloom, keiraknightley]     goreverbinski   \n",
      "2      [danielcraig, christophwaltz, léaseydoux]         sammendes   \n",
      "3      [christianbale, michaelcaine, garyoldman]  christophernolan   \n",
      "4    [taylorkitsch, lynncollins, samanthamorton]     andrewstanton   \n",
      "\n",
      "                              keywords                               genres  \\\n",
      "0     [cultureclash, future, spacewar]         [action, adventure, fantasy]   \n",
      "1     [ocean, drugabuse, exoticisland]         [adventure, fantasy, action]   \n",
      "2     [spy, basedonnovel, secretagent]           [action, adventure, crime]   \n",
      "3  [dccomics, crimefighter, terrorist]               [action, crime, drama]   \n",
      "4      [basedonnovel, mars, medallion]  [action, adventure, sciencefiction]   \n",
      "\n",
      "                                                soup  \n",
      "0  cultureclash future spacewar samworthington zo...  \n",
      "1  ocean drugabuse exoticisland johnnydepp orland...  \n",
      "2  spy basedonnovel secretagent danielcraig chris...  \n",
      "3  dccomics crimefighter terrorist christianbale ...  \n",
      "4  basedonnovel mars medallion taylorkitsch lynnc...  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:31.581092Z",
     "start_time": "2024-12-01T01:41:31.492530Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 加载数据\n",
    "ratings = pd.read_csv('dataset/ratings_small.csv')\n",
    "\n",
    "# 创建用户-电影评分矩阵\n",
    "user_movie_matrix = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "# 转换为矩阵形式\n",
    "matrix = user_movie_matrix.values\n",
    "print(f\"User-Movie Matrix Shape: {matrix.shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Movie Matrix Shape: (671, 9066)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:32.181785Z",
     "start_time": "2024-12-01T01:41:31.597280Z"
    }
   },
   "source": [
    "# SVD 分解\n",
    "u, sigma, vt = svds(matrix, k=50)  # k 是潜在因子数\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# 重构评分矩阵\n",
    "predicted_ratings = np.dot(np.dot(u, sigma), vt)\n",
    "\n",
    "# 将预测评分矩阵转为 DataFrame\n",
    "predicted_ratings_df = pd.DataFrame(predicted_ratings, index=user_movie_matrix.index, columns=user_movie_matrix.columns)\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:32.249111Z",
     "start_time": "2024-12-01T01:41:32.233512Z"
    }
   },
   "source": [
    "def predict_rating(user_id, movie_id):\n",
    "    # 检查是否存在用户和电影\n",
    "    if user_id in predicted_ratings_df.index and movie_id in predicted_ratings_df.columns:\n",
    "        return predicted_ratings_df.loc[user_id, movie_id]\n",
    "    else:\n",
    "        # 如果用户或电影不存在，返回默认值（例如 0 或平均值）\n",
    "        return 0  # 或者可以返回全局平均评分\n"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:41:32.314170Z",
     "start_time": "2024-12-01T01:41:32.297705Z"
    }
   },
   "source": [
    "# 示例\n",
    "user_id = 1\n",
    "movie_id = 110\n",
    "predicted_score = predict_rating(user_id, movie_id)\n",
    "print(f\"Predicted rating for User {user_id} on Movie {movie_id}: {predicted_score:.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for User 1 on Movie 110: 0.12\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:46:39.382846Z",
     "start_time": "2024-12-01T01:46:39.369631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, df, ratings_df, latent_dim=50):\n",
    "        self.df = df\n",
    "        self.ratings_df = ratings_df\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 初始化时进行SVD分解\n",
    "        self._initialize_svd()\n",
    "        # 初始化TF-IDF\n",
    "        self._initialize_tfidf()\n",
    "        \n",
    "    def _initialize_svd(self):\n",
    "        print(\"Initializing SVD...\")\n",
    "        # 创建用户-电影评分矩阵\n",
    "        user_movie_matrix = self.ratings_df.pivot(\n",
    "            index='userId', \n",
    "            columns='movieId', \n",
    "            values='rating'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # 保存用户和电影的映射关系\n",
    "        self.user_ids = user_movie_matrix.index\n",
    "        self.movie_ids = user_movie_matrix.columns\n",
    "        \n",
    "        # SVD分解\n",
    "        matrix = user_movie_matrix.values\n",
    "        self.u, self.sigma, self.vt = svds(matrix, k=self.latent_dim)\n",
    "        print(\"SVD initialization completed\")\n",
    "        \n",
    "    def _initialize_tfidf(self):\n",
    "        print(\"Initializing TF-IDF...\")\n",
    "        # 确保overview列存在且已处理缺失值\n",
    "        if 'overview' not in self.df.columns:\n",
    "            raise ValueError(\"'overview' column not found in movie dataframe\")\n",
    "            \n",
    "        overviews = self.df['overview'].fillna('')\n",
    "        \n",
    "        # 创建和训练TF-IDF向量化器\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        self.tfidf_matrix = tfidf.fit_transform(overviews)\n",
    "        print(\"TF-IDF initialization completed\")\n",
    "    \n",
    "    def _prepare_user_features(self):\n",
    "        # 从ratings中提取用户行为特征\n",
    "        user_stats = self.ratings_df.groupby('userId').agg({\n",
    "            'rating': ['count', 'mean', 'std']\n",
    "        }).fillna(0)\n",
    "        \n",
    "        # 添加来自SVD的用户潜在特征\n",
    "        user_latent = pd.DataFrame(\n",
    "            self.u, \n",
    "            index=self.user_ids,\n",
    "            columns=[f'latent_{i}' for i in range(self.latent_dim)]\n",
    "        )\n",
    "        \n",
    "        # 合并所有用户特征\n",
    "        user_features = pd.concat([user_stats, user_latent], axis=1)\n",
    "        return StandardScaler().fit_transform(user_features)\n",
    "    \n",
    "    def _prepare_movie_features(self):\n",
    "        # 电影基本特征\n",
    "        numeric_features = ['budget', 'popularity', 'vote_average', 'vote_count']\n",
    "        movie_features = self.df[numeric_features].fillna(0)\n",
    "        \n",
    "        # 添加TF-IDF特征\n",
    "        tfidf_features = pd.DataFrame(\n",
    "            self.tfidf_matrix.toarray(),\n",
    "            index=self.df.index\n",
    "        )\n",
    "        \n",
    "        # 添加来自SVD的电影潜在特征\n",
    "        movie_latent = pd.DataFrame(\n",
    "            self.vt.T,\n",
    "            index=self.movie_ids,\n",
    "            columns=[f'latent_{i}' for i in range(self.latent_dim)]\n",
    "        )\n",
    "        \n",
    "        # 合并所有电影特征\n",
    "        movie_features = pd.concat(\n",
    "            [movie_features, tfidf_features, movie_latent],\n",
    "            axis=1\n",
    "        )\n",
    "        return StandardScaler().fit_transform(movie_features)\n",
    "    \n",
    "    def prepare_features(self):\n",
    "        print(\"Preparing features...\")\n",
    "        # 准备用户特征\n",
    "        user_features = self._prepare_user_features()\n",
    "        \n",
    "        # 准备电影特征\n",
    "        movie_features = self._prepare_movie_features()\n",
    "        \n",
    "        print(\"Feature preparation completed\")\n",
    "        return user_features, movie_features\n",
    "        \n",
    "    def train(self, epochs=10, batch_size=64):\n",
    "        print(\"Starting training...\")\n",
    "        # 准备训练数据\n",
    "        user_features, movie_features = self.prepare_features()\n",
    "        \n",
    "        # 创建数据集和数据加载器\n",
    "        dataset = MovieDataset(\n",
    "            user_features, \n",
    "            movie_features,\n",
    "            self.ratings_df['rating'].values / 5.0  # 归一化评分\n",
    "        )\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # 初始化模型\n",
    "        self.model = HybridMLP(\n",
    "            user_features.shape[1], \n",
    "            movie_features.shape[1]\n",
    "        ).to(self.device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        \n",
    "        # 训练循环\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                user_feat = batch['user_features'].to(self.device)\n",
    "                movie_feat = batch['movie_features'].to(self.device)\n",
    "                ratings = batch['rating'].to(self.device)\n",
    "                \n",
    "                # 前向传播\n",
    "                predictions = self.model(user_feat, movie_feat)\n",
    "                loss = criterion(predictions, ratings)\n",
    "                \n",
    "                # 反向传播\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T01:46:54.257316Z",
     "start_time": "2024-12-01T01:46:52.705612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入必要的库\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 创建推荐系统实例\n",
    "recommender = HybridRecommender(df2, ratings)\n",
    "\n",
    "# 训练模型\n",
    "recommender.train(epochs=10, batch_size=64)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SVD...\n",
      "SVD initialization completed\n",
      "Initializing TF-IDF...\n",
      "TF-IDF initialization completed\n",
      "Starting training...\n",
      "Preparing features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['str', 'tuple'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m recommender \u001B[38;5;241m=\u001B[39m HybridRecommender(df2, ratings)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# 训练模型\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[43mrecommender\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[22], line 100\u001B[0m, in \u001B[0;36mHybridRecommender.train\u001B[1;34m(self, epochs, batch_size)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting training...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# 准备训练数据\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m user_features, movie_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprepare_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# 创建数据集和数据加载器\u001B[39;00m\n\u001B[0;32m    103\u001B[0m dataset \u001B[38;5;241m=\u001B[39m MovieDataset(\n\u001B[0;32m    104\u001B[0m     user_features, \n\u001B[0;32m    105\u001B[0m     movie_features,\n\u001B[0;32m    106\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mratings_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m5.0\u001B[39m  \u001B[38;5;66;03m# 归一化评分\u001B[39;00m\n\u001B[0;32m    107\u001B[0m )\n",
      "Cell \u001B[1;32mIn[22], line 89\u001B[0m, in \u001B[0;36mHybridRecommender.prepare_features\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreparing features...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# 准备用户特征\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m user_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_user_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# 准备电影特征\u001B[39;00m\n\u001B[0;32m     92\u001B[0m movie_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_movie_features()\n",
      "Cell \u001B[1;32mIn[22], line 59\u001B[0m, in \u001B[0;36mHybridRecommender._prepare_user_features\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# 合并所有用户特征\u001B[39;00m\n\u001B[0;32m     58\u001B[0m user_features \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([user_stats, user_latent], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mStandardScaler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_features\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:916\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    915\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[1;32m--> 916\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[0;32m    917\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:839\u001B[0m, in \u001B[0;36mStandardScaler.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 839\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:875\u001B[0m, in \u001B[0;36mStandardScaler.partial_fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    843\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001B[39;00m\n\u001B[0;32m    844\u001B[0m \n\u001B[0;32m    845\u001B[0m \u001B[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    872\u001B[0m \u001B[38;5;124;03m    Fitted scaler.\u001B[39;00m\n\u001B[0;32m    873\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    874\u001B[0m first_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples_seen_\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 875\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    880\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfirst_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    881\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    882\u001B[0m n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    884\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:580\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    517\u001B[0m ):\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    584\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    585\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    586\u001B[0m         )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\base.py:441\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    421\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute.\u001B[39;00m\n\u001B[0;32m    422\u001B[0m \n\u001B[0;32m    423\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;124;03m       should set `reset=False`.\u001B[39;00m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reset:\n\u001B[1;32m--> 441\u001B[0m     feature_names_in \u001B[38;5;241m=\u001B[39m \u001B[43m_get_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    442\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_names_in \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    443\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_names_in_ \u001B[38;5;241m=\u001B[39m feature_names_in\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\validation.py:2020\u001B[0m, in \u001B[0;36m_get_feature_names\u001B[1;34m(X)\u001B[0m\n\u001B[0;32m   2018\u001B[0m \u001B[38;5;66;03m# mixed type of string and non-string is not supported\u001B[39;00m\n\u001B[0;32m   2019\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(types) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m types:\n\u001B[1;32m-> 2020\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m   2021\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names are only supported if all input features have string names, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2022\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut your input has \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypes\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m as feature name / column name types. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2023\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you want feature names to be stored and validated, you must convert \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2024\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2025\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2026\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata, or convert them all to a non-string data type.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2027\u001B[0m     )\n\u001B[0;32m   2029\u001B[0m \u001B[38;5;66;03m# Only feature names of all strings are supported\u001B[39;00m\n\u001B[0;32m   2030\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(types) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m types[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstr\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: Feature names are only supported if all input features have string names, but your input has ['str', 'tuple'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
